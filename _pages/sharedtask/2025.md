---
layout: single
title: "BEA 2025 Shared Task"
subtitle: &subtitle "Pedagogical Ability Assessment of AI-powered Tutors" 
permalink: /sharedtask/2025
toc: true
sidebar:
  - title: *subtitle
    image: ""
    image_alt: ""
    text: "
Shared task organized during the [BEA](/bea/2025) workshop at [ACL 2025](https://sig-edu.org/bea/2025).<br>
Vienna, Austria <br>
July 31 and August 1, 2025"
    nav: 
---

<h1>{{ page.subtitle }}</h1>


## Motivation

Conversational agents offer promising opportunities for education as they can fulfill various roles (e.g., intelligent tutors and service-oriented assistants) and pursue different objectives (e.g., improving student skills and increasing instructional efficiency) (Wollny et al., 2021), among which serving as an AI tutor is one of the most prevalent tasks (Tack et al., 2023). Recent advances in the development of Large Language Models (LLMs) provide our field with promising ways of building AI-based conversational tutors, which can generate human-sounding dialogues on the fly. The key questions posed in previous research (Tack and Piech, 2022; Tack et al., 2023), however, remain: _How can we test whether state-of-the-art generative models are good AI teachers, capable of replying to a student in an educational dialogue?_

Evaluating dialogue systems in general presents a significant challenge. While human evaluation is still considered the most reliable method for assessing dialogue quality, its high cost and lack of reproducibility have led to the adaptation of both reference-based and reference-free automatic metrics, originally used in machine translation and
summary evaluation, for dialogue evaluation (Lin, 2004; Popovic, 2017; Post, 2018; Gao et al., 2020; Liu et al., 2023). When it comes to Intelligent Tutoring Systems (ITSs), which also function as dialogue systems with the specific role of acting as tutors, these general metrics are insufficient. In the educational context, we need to assess complex pedagogical aspects and abilities of such systems, ensuring that they provide students with sufficient, helpful, and factually correct guidance and do not simply reveal answers when the student makes a mistake, among other aspects. Therefore, developing automatic metrics to evaluate these nuanced aspects is essential for creating effective and helpful tutoring systems.

Due to the lack of a standardized evaluation taxonomy, previous work has used different criteria for evaluation. For example, Tack and Piech (2022) and Tack et al. (2023) evaluated models’ responses in terms of whether they _speak like a teacher_, _understand a student_, and _help a student_, while in Macina et al. (2023), responses of models
playing roles of tutors were evaluated by human annotators using _coherence_, _correctness_, and _equitable tutoring_. At the same time, Wang et al. (2024) assess _usefulness_, _care_, and _human-likeness_, and Daheim et al. (2024) use _targetedness_, _correctness_, and _actionability_ of a tutor response as quality evaluation criteria. Such lack of standardization makes it difficult to compare different systems, and, therefore, defining evaluation criteria and developing automatic metrics for them is a crucial task for advancing the field, which we aim to address in this task.

## Task Goals & Description

Following the successful BEA 2023 Shared Task on Generating AI Teacher Responses in Educational Dialogues (Tack et al., 2023), we revisit the question of quality assessment of the tutor responses generated with the AI models (specifically, LLMs) in the context of educational dialogues. We believe that (1) the topic is timely and important, and the shared task will attract BEA community attention; (2) LLMs have significantly advanced in the past couple of years, making it important to revisit this topic after the competition run in 2023; and (3) there is a need to establish a pedagogically motivated benchmark for this task. In contrast to [the BEA 2023 shared task](https://sig-edu.org/sharedtask/2023), our focus is not on the generation of educational dialogues using state-of-the-art LLMs, but rather on __comprehensive evaluation of AI-tutor responses using a set of pedagogically motivated metrics__.

More details on the data and evaluation will be released soon.

## Important Dates

All deadlines are 11:59pm UTC-12 (anywhere on earth). Note that at this point some of the deadlines are tentative and may be adjusted later.

- Development data release: March 26, 2025
- Test data release: April 9, 2025
- System testing and submission of results from teams: until April 23, 2025
- Evaluation of the results: by April 30, 2025
- System papers write-up and submission: May 21, 2025
- Notification of acceptance: May 26, 2025
- Final camera-ready submission: June 9, 2025

## Organizers
- Ekaterina Kochmar (MBZUAI)
- Kaushal Kumar Maurya (MBZUAI)
- Kseniia Petukhova (MBZUAI)
- KV Aditya Srivatsa (MBZUAI)
- Justin Vasselli (Nara Institute of Science and Technology)
- Anaïs Tack (KU Leuven)
