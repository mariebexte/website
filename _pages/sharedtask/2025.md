---
layout: single
title: "BEA 2025 Shared Task"
subtitle: &subtitle "Pedagogical Ability Assessment of AI-powered Tutors" 
permalink: /sharedtask/2025
toc: true
sidebar:
  - title: *subtitle
    image: ""
    image_alt: ""
    text: "
Shared task organized during the [BEA](/bea/2025) workshop at [ACL 2025](https://sig-edu.org/bea/2025).<br>
Vienna, Austria <br>
July 31 and August 1, 2025"
    nav: 
---

<h1>{{ page.subtitle }}</h1>


## Motivation

Conversational agents offer promising opportunities for education as they can fulfill various roles (e.g., intelligent tutors and service-oriented assistants) and pursue different objectives (e.g., improving student skills and increasing instructional efficiency) (Wollny et al., 2021), among which serving as an AI tutor is one of the most prevalent tasks (Tack et al., 2023). Recent advances in the development of Large Language Models (LLMs) provide our field with promising ways of building AI-based conversational tutors, which can generate human-sounding dialogues on the fly. The key questions posed in previous research (Tack and Piech, 2022; Tack et al., 2023), however, remain: _How can we test whether state-of-the-art generative models are good AI teachers, capable of replying to a student in an educational dialogue?_

Evaluating dialogue systems in general presents a significant challenge. While human evaluation is still considered the most reliable method for assessing dialogue quality, its high cost and lack of reproducibility have led to the adaptation of both reference-based and reference-free automatic metrics, originally used in machine translation and
summary evaluation, for dialogue evaluation (Lin, 2004; Popovic, 2017; Post, 2018; Gao et al., 2020; Liu et al., 2023). When it comes to Intelligent Tutoring Systems (ITSs), which also function as dialogue systems with the specific role of acting as tutors, these general metrics are insufficient. In the educational context, we need to assess complex pedagogical aspects and abilities of such systems, ensuring that they provide students with sufficient, helpful, and factually correct guidance and do not simply reveal answers when the student makes a mistake, among other aspects. Therefore, developing automatic metrics to evaluate these nuanced aspects is essential for creating effective and helpful tutoring systems.

Due to the lack of a standardized evaluation taxonomy, previous work has used different criteria for evaluation. For example, Tack and Piech (2022) and Tack et al. (2023) evaluated models’ responses in terms of whether they _speak like a teacher_, _understand a student_, and _help a student_, while in Macina et al. (2023), responses of models
playing roles of tutors were evaluated by human annotators using _coherence_, _correctness_, and _equitable tutoring_. At the same time, Wang et al. (2024) assess _usefulness_, _care_, and _human-likeness_, and Daheim et al. (2024) use _targetedness_, _correctness_, and _actionability_ of a tutor response as quality evaluation criteria. Such lack of standardization makes it difficult to compare different systems, and, therefore, defining evaluation criteria and developing automatic metrics for them is a crucial task for advancing the field, which we aim to address in this task.

## Task Goals & Description

Following the successful BEA 2023 Shared Task on Generating AI Teacher Responses in Educational Dialogues (Tack et al., 2023), we revisit the question of quality assessment of the tutor responses generated with the AI models (specifically, LLMs) in the context of educational dialogues. We believe that (1) the topic is timely and important, and the shared task will attract BEA community attention; (2) LLMs have significantly advanced in the past couple of years, making it important to revisit this topic after the competition run in 2023; and (3) there is a need to establish a pedagogically motivated benchmark for this task. In contrast to [the BEA 2023 shared task](https://sig-edu.org/sharedtask/2023), our focus is not on the generation of educational dialogues using state-of-the-art LLMs, but rather on __comprehensive evaluation of AI-tutor responses using a set of pedagogically motivated metrics__.

In this shared task, we will focus on __educational dialogues between a student and a tutor in the mathematical domain grounded in student mistakes or confusion__, where the AI tutor aims to remediate such mistakes or confusions. Dialogues in the datasets provided in this shared task include: 

- The context consisting of several prior turns from both the tutor and the student. These are extracted from two popular datasets of educational dialogues in the mathematical domain – MathDial (Macina et al., 2023) and Bridge (Wang et al., 2024);
- The last utterance from the student containing a mistake; and
- A set of possible responses to the last student’s utterance from a range of LLM-based tutors and, where available, human tutors, aimed at mistake remediation.

The LLM-based tutor responses are generated by the organizers of the shared task using a set of state-of-the-art LLMs of various sizes and capabilities, including: __GPT-4__ (Achiam et al., 2023), __Gemini__ (Reid et al., 2024), __Sonnet__ (Anthropic), __Mistral__ (Jiang et al., 2023), __Llama-3.1-8B__ and __Llama-3.1-405B__ (Dubey et al., 2024), and __Phi-3__ (Abdin et al., 2024).

The __identities of the tutors__ will be included in the development subset provided to the task participants, but __not in the test set__. In addition to the responses themselves, the development subset contains annotation of their quality along the following pedagogically motivated dimensions:

- __Mistake identification__: Since all dialogues in the dataset contain a mistake made by the student, a good quality response from the tutor should include the relevant mistake identification. This corresponds to _student understanding_ in the schema of Tack and Piech (2022) and _correctness_ in the schemata of Macina et al. (2023) and Daheim et al. (2024).
- __Mistake location__: A good tutor response should not only notify the student of the committed error, but also point to its location in the answer and outline what the error is to help student remediate it in their next response. This corresponds to _targetedness_ in Daheim et al. (2024).
- __Providing guidance__: A good tutor response should provide the student with relevant and helpful guidance, such as a hint, an
explanation, or a supporting question. This aspect corresponds to _helping a student_ in Tack and Piech (2022) and _usefulness_ in Wang et al. (2024).
- __Actionability__: Finally, once the guidance is provided to a student, it should be clear from a good tutor response what the student should do next; in other words, the tutor response should not be vague, unclear or a conversation stopper. This aspect in our schema corresponds to _actionability_ in Daheim et al. (2024).

For more details on the annotation and data collection, please refer to Maurya et al. (2025).


## Important Dates

All deadlines are __11:59pm UTC-12 (anywhere on Earth)__. 

- __March 12, 2025__: Development data release
- __April 9, 2025__: Test data release
- __until April 23, 2025__: System testing and submission of results from teams
- __by April 30, 2025__: Evaluation of the results
- __May 21, 2025__: System papers write-up and submission
- __May 28, 2025__: Notification of acceptance
- __June 9, 2025__: Final camera-ready submission
- __July 31 and August 1, 2025__: Workshop at ACL

## FAQ

Questions about this shared task should be sent to [bea.sharedtask.2025@gmail.com](bea.sharedtask.2025@gmail.com). We will share the answers to the frequent questions on this page.

## Organizers
- Ekaterina Kochmar (MBZUAI)
- Kaushal Kumar Maurya (MBZUAI)
- Kseniia Petukhova (MBZUAI)
- KV Aditya Srivatsa (MBZUAI)
- Justin Vasselli (Nara Institute of Science and Technology)
- Anaïs Tack (KU Leuven)

Contact: [bea.sharedtask.2025@gmail.com](bea.sharedtask.2025@gmail.com)

## References

- Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. 2024. _Phi-3 technical report: A highly capable language model locally on your phone_. arXiv preprint arXiv:2404.14219
- Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. _GPT-4 technical report_. arXiv preprint arXiv:2303.08774
- Anthropic. _The Claude 3 Model Family: Opus, Sonnet, Haiku_.
- Nico Daheim, Jakub Macina, Manu Kapur, Iryna Gurevych, and Mrinmaya Sachan. 2024. _Stepwise Verification and Remediation of Student Reasoning Errors with Large Language Model Tutors_. arXiv preprint arXiv:2407.09136
- Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. _The LLaMA 3 herd of models_. arXiv preprint arXiv:2407.21783
- Xiang Gao, Yizhe Zhang, Michel Galley, Chris Brockett, and Bill Dolan. 2020. _Dialogue response ranking training with large-scale human feedback data_. arXiv preprint arXiv:2009.06978
- Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. _Mistral 7B_. arXiv preprint arXiv:2310.06825
- Chin-Yew Lin. 2004. _Rouge: A package for automatic evaluation of summaries_. In Text summarization branches out, pages 74–81
- Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. _G-eval: NLG evaluation using GPT-4 with better human alignment_. arXiv preprint arXiv:2303.16634
- Jakub Macina, Nico Daheim, Sankalan Pal Chowdhury, Tanmay Sinha, Manu Kapur, Iryna Gurevych, and Mrinmaya Sachan. 2023. _MathDial: A dialogue tutoring dataset with rich pedagogical properties grounded in math reasoning problems_. arXiv preprint arXiv:2305.14536.
- Maja Popovic. 2017. _chrF++: words helping character n-grams_. In Proceedings of the second conference on machine translation, pages 612–618
- Kaushal Kumar Maurya, KV Srivatsa, Kseniia Petukhova, and Ekaterina Kochmar. _Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical Ability Assessment of LLM-Powered AI Tutors_. In Proceedings of NAACL 2025 (main).
- Matt Post. 2018. _A call for clarity in reporting BLEU scores_. arXiv preprint arXiv:1804.08771
- Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-Baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. 2024. _Gemini 1.5: Unlocking multimodal understanding across millions of
tokens of context_. arXiv preprint arXiv:2403.05530
- Anaïs Tack and Chris Piech. 2022. _The AI teacher test: Measuring the pedagogical ability of blender and GPT-3 in educational dialogues_. arXiv preprint arXiv:2205.07540
- Anaïs Tack, Ekaterina Kochmar, Zheng Yuan, Serge Bibauw, and Chris Piech. 2023. _The BEA 2023 shared task on generating AI teacher responses in educational dialogues_. arXiv preprint arXiv:2306.06941.
- Rose Wang, Qingyang Zhang, Carly Robinson, Susanna Loeb, and Dorottya Demszky. 2024. _Bridging the novice-expert gap via models of decision-making: A case study on remediating math mistakes_. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 2174–2199
- Wollny, Sebastian & Schneider, Jan & Di Mitri, Daniele & Weidlich, Joshua & Rittberger, Marc & Drachsler, Hendrik. 2021. _Are We There Yet? - A Systematic Literature Review on Chatbots in Education_. Frontiers in Artificial Intelligence 4. 654924. (doi:10.3389/frai.2021.654924)
